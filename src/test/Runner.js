/* Daft Toolkit                         http://www.measurement-factory.com/
 * Copyright (C) 2015,2016 The Measurement Factory.
 * Licensed under the Apache License, Version 2.0.                       */

/* Manages [concurrent] execution of tests. */

// Terminology:
//
// A lot of things are often called "test". Daft sources use this and related
// terms inconsistently. The summary below documents terminology we want to
// use going forward, but that terminology snapshot itself has problems, and
// some code (especially code outside this source file) deviates from this
// particular documentation snapshot.
//
// * Test object: An instance of a class derived from test/Test. Immediately
//   after a test object is created, the corresponding DUT instance is started
//   and configured (see Test::startup()). Just before a test object is
//   discarded, the corresponding DUT instance is shut down (see
//   Test::shutdown()).
//
// * Test thread: Verification of a configured-and-running DUT instance using
//   a single test_object.run() call.
//
// * Test attempt: Concurrent test threads sharing the same test object. Test
//   object lifetime is one test attempt.
//
// * Test: Sequential test attempts sharing the same Test class (i.e. a class
//   derived from tests/Test). Test attempts stop upon the first successful
//   attempt or when reaching the retry limit. Config stays constant during a
//   test.
//
// * Daft run: Sequential tests sharing the same Test class, each testing one
//   global configuration (i.e. Config) generated by Test::Configurators().

import assert from "assert";
import TestRun from "./Run";
import * as Config from "../misc/Config";
import * as Gadgets from "../misc/Gadgets";

Config.Recognize([
    {
        option: "concurrency-level",
        type: "Number",
        default: "1",
        description: "the number of threads to start for each test attempt",
    },
    {
        option: "tests",
        type: "Number",
        description: "artificially limits or inflates the number of tests",
    },
    {
        option: "retries",
        type: "Number",
        default: "0",
        description: "the number of consecutive test attempt failures to retry; " +
            "a retry is not counted as a new test",
    },
]);

let TestsStarted = 0; // the number of tests started
let TotalAttemptsMade = 0; // the number of test attempts started, across all tests

let ThreadsRunning = 0; // number of concurrent tests running now

// executes a single test thread
async function _TestThread(test, threadRun) {
    try {
        ++ThreadsRunning;
        console.log(`Starting ${threadRun}. Concurrency level: ${ThreadsRunning}`);
        await test.run(threadRun);
    }
    finally {
        --ThreadsRunning;
        console.log(`Finished ${threadRun}. Concurrency level: ${ThreadsRunning}`);
    }
}

// executes a single test attempt
async function _TestAttempt(Test, attemptId) {

    const attemptRun = new TestRun(TestsStarted);
    attemptRun.setAttempt(attemptId, Config.Retries);

    const test = new Test();
    await test.startup();

    try {
        let threads = [];

        if (Config.ConcurrencyLevel > 1)
            console.log(`Starting ${Config.ConcurrencyLevel} test threads`);

        for (let threadId = 1; threadId <= Config.ConcurrencyLevel; ++threadId) {
            const threadRun = attemptRun.clone();
            threadRun.setThread(threadId, Config.ConcurrencyLevel);
            threads.push(_TestThread(test, threadRun));
        }

        if (Config.ConcurrencyLevel > 1)
            console.log(`Started all ${Config.ConcurrencyLevel} test threads.`);

        await Promise.all(threads);

        if (Config.ConcurrencyLevel > 1)
            console.log(`Finished all ${Config.ConcurrencyLevel} test threads.`);
    } catch (error) {
        // We report the caught `try` error A here because otherwise, if
        // `finally` code throws B, then error A is not reported! The same
        // loss would happen if a global timeout expires (throwing C) while
        // waiting for that `finally` code below to complete.
        console.log('Test failure: ', error);
        // Signal an error to caller without reporting the same error twice.
        throw "Test failure was detailed earlier";
    } finally {
        console.log(`Shutting down DUT`);
        await test.shutdown();
    }
}

// executes a single test (i.e. all test attempts)
async function _TestCurrentConfig(Test) {
    ++TestsStarted;
    let attemptsMade = 0;
    let attemptsFailed = 0;
    const attemptsAllowed = 1 + Config.Retries;
    for (let attemptId = 1; attemptsMade < attemptsAllowed; ++attemptId) {
        try {
            ++TotalAttemptsMade;
            ++attemptsMade;
            await _TestAttempt(Test, attemptId);
            break; // stop on success
        }
        catch (error) {
            ++attemptsFailed;
            if (attemptsMade < attemptsAllowed) {
                const progress = attemptsFailed + "/" + attemptsAllowed;
                console.log(`Test attempt failure (${progress}):`, error);
                // keep going
            } else {
                console.log(`Test failure`);
                throw error;
            }
        }
    }

    // success
    if (attemptsMade > 1)
        console.log("Probability of a test failure:", attemptsFailed, "/", attemptsMade, "=", Gadgets.PrettyPercent(attemptsFailed, attemptsMade));
}

// generates and tests all configurations using a given Test class
export default async function Run(Test) {
    const userConfig = Config.clone();
    const configurators = Test.Configurators();
    const totalConfigs = configurators.length;
    console.log("Planned test configurations:", totalConfigs);
    // the Test module must provide at least one, possibly empty, configurator
    assert.notStrictEqual(totalConfigs, 0);

    // by default, test each configuration once (concurrently if needed)
    const limitedTests = Config.Tests !== undefined;
    // the two constants below are only meaningful if limitedTests
    // TODO: Consider switching to a simpler model where we pass through each
    // config once before circling back to the first config!
    const defaultTests = totalConfigs;
    const plannedTests = Config.Tests; // may be undefined!

    // tests that cannot be spread across all configurations evenly
    let leftoverTests = 0;
    if (limitedTests) {
        if (plannedTests < defaultTests) {
            console.log(`Warning: --tests ${plannedTests} is too small; need ` +
                `${totalConfigs} tests to test each configuration once`);
        } else {
            leftoverTests = plannedTests % defaultTests;
        }
    }

    let generatedConfigs = 0;
    for (const configurator of configurators) {
        // stop generating configurations if the test limit was reached
        if (limitedTests && TestsStarted >= plannedTests) {
            console.log("Warning: Reached --tests limit before testing all",
                totalConfigs, "test configurations:",
                TestsStarted, ">=", plannedTests);
            break;
        }

        configurator.forEach(step => step(Config));
        ++generatedConfigs;
        console.log(`Test configuration #${generatedConfigs}:\n${Config.sprint()}`);

        // the number of tests we want to run for the current test configuration
        let testsForCurrentConfig = 1; // may be increased below
        if (limitedTests && plannedTests > defaultTests) {
            testsForCurrentConfig = Math.trunc(plannedTests / totalConfigs);
            assert(testsForCurrentConfig >= 1);
            if (leftoverTests > 0) {
                ++testsForCurrentConfig;
                --leftoverTests;
            }
        }
        console.log("Tests for this configuration:", testsForCurrentConfig);
        assert(testsForCurrentConfig > 0);

        // run one or more sequential tests of the current configuration
        for (let t = 0; t < testsForCurrentConfig; ++t)
            await _TestCurrentConfig(Test);

        Config.reset(userConfig);
    }

    if (totalConfigs === generatedConfigs)
        console.log("Tested all", totalConfigs, "configurations");
    else
        console.log("Tested only", generatedConfigs, "out of", totalConfigs, "configurations");

    if (generatedConfigs !== TestsStarted)
        console.log("Ran", TestsStarted, "tests");

    if (TestsStarted !== TotalAttemptsMade)
        console.log("Made a total of", TotalAttemptsMade, "test attempts");
}
